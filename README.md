# NtechLab_DS_internship
1. Подготовка данных
Для обучения и тестирования бы предоставлен набор из 100 тысяч картинок, которые находились в двух папках по классам (пол: мужской, женский). Для того чтобы разделить данные на тренировочный и валидационный сеты был написан код, который находится в первой ячейке тренировочного notebook-файла: созданы соответсвующие директории, получены списки всех изображений каждого класса, после чего с помощью функции split из библиотеки numpy списки были разбиты на тренировочный/валидационный в соотношений 90%/10%, изображения скопированы в соответствующие директории. Было принято решение собрать данные в csv файлы с помощью библиотеки pandas: в первый столбец записывались имена изображений с приставкой названия папки перед ними, во второй метку класса: 0 - мужской пол, 1 - женский. Для того чтобы работать с полученными csv файлами был создан класс для кастомного датасета, путём переопределения методов класса Dataset из PyTorch, также были добавлены классы для смены размера изображения, перевода в PyTorch тензор и нормализации.
2. Используемая нейросеть
Первой неросетью для решения задачи была выбра нейросеть VGG16, так как я уже использовал её в дипломной работе и она отлична показывала себе в решении задач распознавания изображений. После пробной тренировки было выявлено что модель нейросети имеет большой вес (1 Гб) и скорость работы, с целью выяснения более подходящей для данной задачи нейросети я просмотрел показатели top-1 и top-5 ошибок предобученных нейросетей, предоставлямых PyTorch и примерный размер обученных моделей. Оптимальным вариантом была выбрана нейросеть ResNet50. У данной нейросети был перопределен последний решающий полносвязный слой так чтобы выходом сети были метки 2 классов. Нейросеть была преобучены на датасете ImageNet, поэтому перед переобучением все входных изображения необходимо было нормализовать с теми же параметрами стандартного и среднего отклонения. При выборе метода переобучени из предоставленных feature extracting  и fine-tuning, после тестов был выбран второй, так как точность обученной модели данным методом выше.
3. Параметры обучения и полученные результаты
Для выбора оптимального размера партии были проведены тесты на рамерах 8, 16 и 32, в которых выяснилось что наибольшая точность достигается при размере партии в 16 изображений. При выборе оптимизатора между SGD и Adam лучше оказался первый. Параметр скорости обучения и момента были установлены стандартные 0.001 и 0.9 соответственно. При проведении обучения было выясненно что уже после 2 эпох достигается максимальная точность распознавания, в последующих эпохах наблюдается переобучение, так как на тестовых данных точность увеличивается и потери снижаются при уменьшении точности на валидационном наборе и увелечения потерь на нём. Для подсчета потерь использовалась функция кросс-энтропии. Максимальная полученная точность на валидационном датасете: 0.9848, потери: 0.0461.

Для запуска тренировки нейросети необходимо распаковать предоставленный изначальный архив, и запустить все ячейки notebook-файла (необходимо чтобы файл находился в той же директории что и папка с изображенями) или скачать по ссылке предоставленной ниже архив с уже разбитыми на сеты изображенями, распаковать его и запустить файл со второй ячейки. Для запуска распознавания использовать разработанный скрипт которому необходимо передать в качестве аргумента путь к папку с изображенями, пример вызова: python3 process.py folder/to/process/. Результаты работы скрипта сохраняются в файл process_results.json (необходимо чтобы файл модели находился п той же директории что и папка с изображениями и скрипт). Обучение производилось на сервисе Google Colab с использованием GPU.
Ссылка на обученную модель: https://drive.google.com/file/d/1lsG8vCNHUzwkTLt3J9Hzj15FBqDttLd2/view?usp=sharing
Ссылка на скриншоты с тестированием различных параметров обучения: https://drive.google.com/file/d/1XkmViYsKnhCYNyFjTrbNRCaca3bSxR4G/view?usp=sharing
Ссылка на датасет, разбитый на тренировочный/валидационный: https://drive.google.com/file/d/1ZhsNo76cBio4a__2NDRMGE46aiExq3xF/view?usp=sharing
